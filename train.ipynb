{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "julian-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vulnerable-incidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "neutral-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "finite-qualification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exist, images will be written in same folder\n",
      "Directory already exist, images will be written in same folder\n",
      "Directory already exist, images will be written in same folder\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import os,glob\n",
    "\n",
    "from os import listdir,makedirs\n",
    "\n",
    "from os.path import isfile,join\n",
    "path = 'E:/Python/derma_disease_dataset/dataset/train/melanoma' # Source Folder\n",
    "dstpath = 'E:/Python/derma_disease_dataset/dataset/train2/melanoma' # Destination Folder\n",
    "path2 = 'E:/Python/derma_disease_dataset/dataset/train/nevus'\n",
    "dstpath2 = 'E:/Python/derma_disease_dataset/dataset/train2/nevus'\n",
    "path3 = 'E:/Python/derma_disease_dataset/dataset/train/seborrheic_keratosis'\n",
    "dstpath3 = 'E:/Python/derma_disease_dataset/dataset/train2/seborrheic_keratosis'\n",
    "try:\n",
    "    makedirs(dstpath)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")\n",
    "try:\n",
    "    makedirs(dstpath2)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")\n",
    "try:\n",
    "    makedirs(dstpath3)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")\n",
    "# Folder won't used\n",
    "files = list(filter(lambda f: isfile(join(path,f)), listdir(path)))\n",
    "files2 = list(filter(lambda f2: isfile(join(path2,f2)), listdir(path2)))\n",
    "files3 = list(filter(lambda f3: isfile(join(path3,f3)), listdir(path3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "toxic-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in listdir(path) if isfile(join(path,f))] \n",
    "files2 = [f2 for f2 in listdir(path2) if isfile(join(path2,f2))]\n",
    "files3 = [f3 for f3 in listdir(path3) if isfile(join(path3,f3))]\n",
    "filter = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]);\n",
    "for image in files:\n",
    "    try:\n",
    "        img = cv2.imread(os.path.join(path,image))\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_img_1=cv2.filter2D(gray,-1,filter)\n",
    "        #dt =  cv2.GaussianBlur(gray, (1,1),0)\n",
    "        dstPath = join(dstpath,image)\n",
    "        cv2.imwrite(dstPath,sharpen_img_1)\n",
    "    except:\n",
    "        print (\"{} is not converted\".format(image))\n",
    "for image2 in files2:\n",
    "    try:\n",
    "        img2 = cv2.imread(os.path.join(path2,image2))\n",
    "        gray2 = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_img_2=cv2.filter2D(gray2,-1,filter)\n",
    "        #dt2 =  cv2.GaussianBlur(gray2, (1,1),0)\n",
    "        dstPath2 = join(dstpath2,image2)\n",
    "        cv2.imwrite(dstPath2,sharpen_img_2)\n",
    "    except:\n",
    "        print (\"{} is not converted\".format(image2))   \n",
    "for image3 in files3:\n",
    "    try:\n",
    "        img3 = cv2.imread(os.path.join(path3,image3))\n",
    "        gray3 = cv2.cvtColor(img3,cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_img_3=cv2.filter2D(gray3,-1,filter)\n",
    "        #dt3 =  cv2.GaussianBlur(gray3, (1,1),0)\n",
    "        dstPath3 = join(dstpath3,image3)\n",
    "        cv2.imwrite(dstPath3,sharpen_img_3)\n",
    "    except:\n",
    "        print (\"{} is not converted\".format(image2))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "positive-extraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exist, images will be written in same folder\n",
      "Directory already exist, images will be written in same folder\n",
      "Directory already exist, images will be written in same folder\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-520ffccffe51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Directory already exist, images will be written in same folder\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Folder won't used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mfiles4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf4\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mfiles5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf5\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mfiles6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mf6\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import os,glob\n",
    "\n",
    "from os import listdir,makedirs\n",
    "\n",
    "from os.path import isfile,join\n",
    "path4 = 'E:/Python/derma_disease_dataset/dataset/test/melanoma' # Source Folder\n",
    "dstpath4 = 'E:/Python/derma_disease_dataset/dataset/test2/melanoma' # Destination Folder\n",
    "path5 = 'E:/Python/derma_disease_dataset/dataset/test/nevus'\n",
    "dstpath5 = 'E:/Python/derma_disease_dataset/dataset/test2/nevus'\n",
    "path6 = 'E:/Python/derma_disease_dataset/dataset/test/seborrheic_keratosis'\n",
    "dstpath6 = 'E:/Python/derma_disease_dataset/dataset/test2/seborrheic_keratosis'\n",
    "try:\n",
    "    makedirs(dstpath4)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")\n",
    "try:\n",
    "    makedirs(dstpath5)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")\n",
    "try:\n",
    "    makedirs(dstpath6)\n",
    "except:\n",
    "    print (\"Directory already exist, images will be written in same folder\")\n",
    "# Folder won't used\n",
    "files4 = list(filter(lambda f4: isfile(join(path4,f4)), listdir(path4)))\n",
    "files5 = list(filter(lambda f5: isfile(join(path5,f5)), listdir(path5)))\n",
    "files6 = list(filter(lambda f6: isfile(join(path6,f6)), listdir(path6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-sunrise",
   "metadata": {},
   "outputs": [],
   "source": [
    "files4 = [f4 for f4 in listdir(path4) if isfile(join(path4,f4))] \n",
    "files5 = [f5 for f5 in listdir(path5) if isfile(join(path5,f5))]\n",
    "files6 = [f6 for f6 in listdir(path6) if isfile(join(path6,f6))]\n",
    "filter = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]]);\n",
    "for image4 in files4:\n",
    "    try:\n",
    "        img4 = cv2.imread(os.path.join(path4,image4))\n",
    "        gray4 = cv2.cvtColor(img4,cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_img_4=cv2.filter2D(gray4,-1,filter)\n",
    "        #dt4 =  cv2.GaussianBlur(gray4, (1,1),0)\n",
    "        dstPath4 = join(dstpath4,image4)\n",
    "        cv2.imwrite(dstPath4,sharpen_img_4)\n",
    "    except:\n",
    "        print (\"{} is not converted\".format(image4))\n",
    "for image5 in files5:\n",
    "    try:\n",
    "        img5 = cv2.imread(os.path.join(path5,image5))\n",
    "        gray5 = cv2.cvtColor(img5,cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_img_5=cv2.filter2D(gray5,-1,filter)\n",
    "        #dt5 =  cv2.GaussianBlur(gray5, (1,1),0)\n",
    "        dstPath5 = join(dstpath5,image5)\n",
    "        cv2.imwrite(dstPath5,sharpen_img_5)\n",
    "    except:\n",
    "        print (\"{} is not converted\".format(image5))   \n",
    "for image6 in files6:\n",
    "    try:\n",
    "        img6 = cv2.imread(os.path.join(path6,image6))\n",
    "        gray6 = cv2.cvtColor(img6,cv2.COLOR_BGR2GRAY)\n",
    "        sharpen_img_6=cv2.filter2D(gray6,-1,filter)\n",
    "        #dt6 =  cv2.GaussianBlur(gray6, (1,1),0)\n",
    "        dstPath6 = join(dstpath6,image6)\n",
    "        cv2.imwrite(dstPath6,sharpen_img_6)\n",
    "    except:\n",
    "        print (\"{} is not converted\".format(image6))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fourth-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='E:/Python/derma_disease_dataset/dataset/train2'\n",
    "test_path='E:/Python/derma_disease_dataset/dataset/test2'\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "super-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confidential-uncle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['melanoma', 'nevus', 'seborrheic_keratosis']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "personal-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (256,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "realistic-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "thermal-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optmizer and loss function\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "organized-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "german-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the size of training and testing images\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "altered-warrant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938 368\n"
     ]
    }
   ],
   "source": [
    "print(train_count,test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "binding-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: tensor(12.3131)\n",
      " Test Accuracy: 0.0\n",
      "Epoch: 1 Train Loss: tensor(5.3023)\n",
      " Test Accuracy: 0.4483695652173913\n",
      "Epoch: 2 Train Loss: tensor(4.4356)\n",
      " Test Accuracy: 0.532608695652174\n",
      "Epoch: 3 Train Loss: tensor(3.3438)\n",
      " Test Accuracy: 0.6086956521739131\n",
      "Epoch: 4 Train Loss: tensor(1.4416)\n",
      " Test Accuracy: 0.6086956521739131\n",
      "Epoch: 5 Train Loss: tensor(0.8489)\n",
      " Test Accuracy: 0.6086956521739131\n",
      "Epoch: 6 Train Loss: tensor(0.4038)\n",
      " Test Accuracy: 0.625\n",
      "Epoch: 7 Train Loss: tensor(0.2549)\n",
      " Test Accuracy: 0.625\n",
      "Epoch: 8 Train Loss: tensor(0.1426)\n",
      " Test Accuracy: 0.625\n",
      "Epoch: 9 Train Loss: tensor(0.1696)\n",
      " Test Accuracy: 0.6385869565217391\n",
      "Epoch: 10 Train Loss: tensor(0.0387)\n",
      " Test Accuracy: 0.6385869565217391\n",
      "Epoch: 11 Train Loss: tensor(0.0326)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 12 Train Loss: tensor(0.0129)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 13 Train Loss: tensor(0.0150)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 14 Train Loss: tensor(0.0106)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 15 Train Loss: tensor(0.0102)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 16 Train Loss: tensor(0.0019)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 17 Train Loss: tensor(0.0013)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 18 Train Loss: tensor(0.0010)\n",
      " Test Accuracy: 0.6494565217391305\n",
      "Epoch: 19 Train Loss: tensor(0.0011)\n",
      " Test Accuracy: 0.6494565217391305\n"
     ]
    }
   ],
   "source": [
    "#Model training and saving best model\n",
    "\n",
    "best_accuracy=0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    #Evaluation and training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs=model(images)\n",
    "        loss=loss_function(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        train_loss+= loss.cpu().data*images.size(0)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        \n",
    "        train_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "        \n",
    "    train_accuracy=train_accuracy/train_count\n",
    "    train_loss=train_loss/train_count\n",
    "    \n",
    "    \n",
    "    # Evaluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy=0.0\n",
    "    for i, (images,labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images=Variable(images.cuda())\n",
    "            labels=Variable(labels.cuda())\n",
    "            \n",
    "        outputs=model(images)\n",
    "        _,prediction=torch.max(outputs.data,1)\n",
    "        test_accuracy+=int(torch.sum(prediction==labels.data))\n",
    "    \n",
    "    test_accuracy=test_accuracy/test_count\n",
    "    \n",
    "    \n",
    "    print('Epoch: '+str(epoch)+' Train Loss: '+str(train_loss))\n",
    "    print(' Test Accuracy: '+str(best_accuracy))\n",
    "    \n",
    "    #Save the best model\n",
    "    if test_accuracy>best_accuracy:\n",
    "        torch.save(model.state_dict(),'best_checkpoint.model')\n",
    "        best_accuracy=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "labeled-anthropology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best Test Accuracy is: 65.22%\n"
     ]
    }
   ],
   "source": [
    "print(' Best Test Accuracy is: {}%'.format(round((best_accuracy*100),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-climate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
